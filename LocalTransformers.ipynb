{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b43497ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pytorch_pretrained_bert pytorch-nlp\n",
    "#pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0323ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge gensim\n",
    "#pip install --upgrade gensim   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61057f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ea787ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a84638c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = text\n",
    "w2v_model = Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ac1e6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'e', 'n', 'i', 's', 'a', 'r', 't', 'b']\n"
     ]
    }
   ],
   "source": [
    "words = list(w2v_model.wv.index_to_key)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a6737fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save('model.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4ad6667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=9, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b483573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "from torch import nn\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79fae40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#from transformers import BertTokenizer\n",
    "#tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54e3935d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.29.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import __version__; print(__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f543b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd9da0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text after tokenization: \n",
      "['after', 'stealing', 'money', 'from', 'the', 'bank', 'vault', ',', 'the', 'bank', 'robber', 'was', 'seen', 'fishing', 'on', 'the', 'mississippi', 'river', 'bank', '.']\n",
      "After adding [CLS] and [SEP]: \n",
      "['[CLS]', 'after', 'stealing', 'money', 'from', 'the', 'bank', 'vault', ',', 'the', 'bank', 'robber', 'was', 'seen', 'fishing', 'on', 'the', 'mississippi', 'river', 'bank', '.', '[SEP]']\n",
      "After converting Tokens to Id: \n",
      "[101, 2044, 11065, 2769, 2013, 1996, 2924, 11632, 1010, 1996, 2924, 27307, 2001, 2464, 5645, 2006, 1996, 5900, 2314, 2924, 1012, 102]\n",
      "tokens: \n",
      "[101, 2044, 11065, 2769, 2013, 1996, 2924, 11632, 1010, 1996, 2924, 27307, 2001, 2464, 5645, 2006, 1996, 5900, 2314, 2924, 1012, 102, 0, 0, 0]\n",
      "Pad Masking: \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "Segment Ids: \n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "tokenize_ = tokenizer.tokenize(text)\n",
    "print(\"Text after tokenization: \")\n",
    "print(tokenize_)\n",
    "max_len = 25\n",
    "\n",
    "textLst = tokenize_[:max_len-2]\n",
    "input_sequence = [\"[CLS]\"] + textLst + [\"[SEP]\"]\n",
    "pad_len = max_len - len(input_sequence)\n",
    "\n",
    "print(\"After adding [CLS] and [SEP]: \")\n",
    "print(input_sequence)\n",
    "tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "print(\"After converting Tokens to Id: \")\n",
    "print(tokens)\n",
    "tokens += [0] * pad_len\n",
    "print(\"tokens: \")\n",
    "print(tokens)\n",
    "pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "print(\"Pad Masking: \")\n",
    "print(pad_masks)\n",
    "segment_ids = [0] * max_len\n",
    "print(\"Segment Ids: \")\n",
    "print(segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31cb81c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'after', 'stealing', 'money', 'from', 'the', 'bank', 'vault', ',', 'the', 'bank', 'robber', 'was', 'seen', 'fishing', 'on', 'the', 'mississippi', 'river', 'bank', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0904d45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 662804195/662804195 [08:12<00:00, 1346438.48B/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e743b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 12, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "\n",
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49775178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores the token vectors, with shape [23 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [23 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd753cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 22 x 768\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [23 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [23 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2a087f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `encoded_layers` has shape [12 x 1 x 23 x 768]\n",
    "\n",
    "# `token_vecs` is a tensor with shape [23 x 768]\n",
    "token_vecs = encoded_layers[11][0]\n",
    "\n",
    "# Calculate the average of all 23 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c145f722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final sentence embedding vector of shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "380bed61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.1358e-01, -6.3680e-01,  6.5889e-01,  1.8925e-02, -3.9966e-01,\n",
       "        -3.7560e-01, -4.7043e-02, -9.8560e-02,  1.5726e-01,  5.8103e-01,\n",
       "        -2.9570e-01,  1.9844e-01,  3.5090e-01,  2.4400e-01,  1.6944e-01,\n",
       "        -3.4082e-01,  2.2248e-01,  1.9042e-01, -7.6469e-02,  5.7022e-01,\n",
       "        -2.9180e-01,  7.1141e-02,  2.5399e-01,  5.2583e-01,  1.1742e-01,\n",
       "         5.8425e-02, -1.4689e-01,  3.7150e-01,  7.4130e-02, -9.4014e-01,\n",
       "         3.2702e-01,  8.9699e-02,  5.6249e-02,  4.0415e-01, -1.9911e-02,\n",
       "         1.4569e-01, -3.2897e-01,  1.0943e-01, -5.1025e-02, -7.1243e-01,\n",
       "         1.2372e-01, -6.4714e-02, -1.0946e-01,  2.0264e-01, -1.5510e-01,\n",
       "        -8.0698e-01,  3.9742e-01,  1.5526e-01, -3.4541e-01,  2.1713e-01,\n",
       "        -3.1845e-02, -3.1141e-01,  3.5776e-01, -2.0226e-01, -4.1421e-01,\n",
       "        -1.7628e-05,  2.1077e-01,  1.1151e-01,  2.2985e-02,  2.5953e-02,\n",
       "         1.0885e-01,  3.1563e-01, -1.3581e-02,  1.4987e-01, -3.4291e-01,\n",
       "         6.5344e-01, -8.8557e-01,  9.2735e-02,  9.0764e-02,  2.6808e-02,\n",
       "         2.3409e-01,  3.0933e-01,  7.3526e-01,  3.7550e-02, -2.0671e-01,\n",
       "         5.5031e-01,  7.4972e-01, -1.8407e-01, -5.2186e-01, -4.1036e-01,\n",
       "        -2.0073e-01, -1.1580e-01, -1.7148e-01, -9.0325e-02,  1.1528e-01,\n",
       "        -1.0974e-01,  1.9958e-01, -1.0975e-02,  4.3271e-01, -4.5817e-01,\n",
       "        -2.0862e-01, -1.9496e-01, -2.0854e-02,  4.9417e-03, -4.3874e-01,\n",
       "         4.0524e-01, -2.9578e-01, -2.7446e-01, -1.2906e-01,  9.8890e-03,\n",
       "        -3.3849e-01, -1.8583e-01,  1.4168e-01,  2.4928e-01,  4.6681e-01,\n",
       "         1.1536e-01,  2.0260e-02,  2.1669e-01,  1.5527e-01, -1.4953e-01,\n",
       "         4.9106e-01, -2.6157e-01,  1.1666e-01, -9.6519e-02, -3.4282e-02,\n",
       "        -1.2650e-01, -8.8032e-02,  3.3668e-01,  6.0603e-01,  2.0547e-01,\n",
       "         2.8955e-02, -1.7510e-01,  3.3316e-01, -3.5487e-01,  2.4917e-01,\n",
       "         6.6935e-02,  4.9404e-02, -5.3654e-01,  1.1896e+00,  7.1951e-02,\n",
       "        -3.7636e-01, -3.8172e-02, -3.5206e-01, -7.0429e-02, -4.7643e-02,\n",
       "        -2.0025e-01, -9.1562e-02, -8.3198e-02, -2.2889e-01,  1.3494e-01,\n",
       "        -3.3458e-01,  1.4655e-01, -3.1977e-01, -2.0987e-01, -5.1173e-03,\n",
       "         2.9897e-01, -2.2214e-01, -1.4237e-01, -5.2862e-01, -1.1260e-01,\n",
       "        -2.6512e-01, -1.5286e-01,  3.5095e-01, -3.8123e-01, -8.6756e-01,\n",
       "         6.8352e-02,  4.3756e-02,  8.6243e-02, -1.3997e-01,  1.7147e-01,\n",
       "        -8.2743e-02,  1.2444e-01, -7.7465e-01,  6.1034e-01, -1.6485e-01,\n",
       "        -2.5099e-01,  1.3876e-01,  2.3906e-01, -1.9737e-01,  3.4331e-01,\n",
       "         2.5294e-01,  1.9859e-01, -2.2997e-01,  5.1155e-02,  2.8950e-01,\n",
       "         7.2038e-02,  2.7841e-01, -1.2340e-01, -3.0461e-01,  1.3556e-01,\n",
       "         1.2714e-01,  2.4105e-02, -1.4891e-01, -3.0615e-01,  2.7076e-01,\n",
       "        -3.2147e-01,  2.8553e-01, -2.0613e-01, -2.2274e-01, -2.6898e-01,\n",
       "         3.3004e-02,  2.3819e-02, -1.8020e-01, -1.9527e-01,  4.1743e-02,\n",
       "        -3.9644e-02, -5.4007e-02, -1.5801e-01,  7.8635e-02, -2.2382e-01,\n",
       "        -9.1878e-02, -8.5740e-02, -3.0624e-02, -3.0870e-01,  4.5771e-01,\n",
       "         3.6771e-01,  1.4609e-01, -3.2466e-01,  4.7040e-02,  3.5385e-01,\n",
       "         1.9661e-01, -1.3200e-01,  3.6517e-01,  4.3081e-01,  4.8659e-01,\n",
       "         4.2864e-01, -7.0406e-02, -1.3079e-01,  8.0647e-01,  9.7799e-02,\n",
       "        -4.7741e-02, -1.4100e-01, -7.1316e-01,  2.9345e-01, -6.9669e-01,\n",
       "         1.8182e-03, -2.4395e-01,  1.6388e-01, -8.0165e-02,  1.7572e-01,\n",
       "        -4.6208e-01, -4.6166e-01, -3.0606e-01,  4.1978e-01, -2.1769e-01,\n",
       "         5.0361e-01, -3.0617e-01,  1.9671e-01,  2.2211e-01, -2.1181e-01,\n",
       "         2.7262e-01,  2.8096e-01,  3.0902e-01, -5.6918e-01,  3.8839e-01,\n",
       "         7.5157e-01, -7.5657e-01, -3.6458e-01,  9.8137e-02, -2.8245e-02,\n",
       "         1.5722e-01, -6.4899e-02,  2.5564e-01, -8.6819e-02,  1.2782e-01,\n",
       "         4.7380e-01, -1.5597e-01, -5.8244e-02,  5.6777e-01, -4.5102e-01,\n",
       "        -5.0045e-01, -4.7923e-01, -6.3895e-02, -2.1446e-01, -2.1325e-01,\n",
       "        -7.6493e-02,  1.4370e-01,  1.8587e-01,  9.2089e-02, -4.2004e-01,\n",
       "        -5.5235e-02,  3.1258e-01,  4.1617e-01, -5.1402e-01,  3.4370e-01,\n",
       "        -1.0430e-01,  2.1275e-02, -3.4457e-01, -8.7189e-01, -1.1301e-01,\n",
       "        -3.3829e-01, -1.3695e-01,  3.2776e-01,  3.3614e-01,  3.2912e-01,\n",
       "        -2.6121e-01,  3.1758e-01,  1.2123e-01, -7.3108e-02, -1.7168e-01,\n",
       "        -7.0312e-02, -3.6211e-01, -3.6451e-01,  4.3132e-01,  6.6263e-02,\n",
       "        -2.9233e-01, -1.8270e-01, -6.0695e-02, -9.9081e-02,  1.8273e-02,\n",
       "         5.0092e-01,  1.0472e-01, -1.4071e-01, -1.2466e-01, -2.3192e-01,\n",
       "        -4.1589e-02,  1.3503e-01, -6.7168e-01,  8.1334e-01,  3.6695e-01,\n",
       "        -2.0652e-01,  1.2791e-01, -2.6423e-01,  1.7442e-01, -1.0992e-01,\n",
       "         4.9488e-01, -3.7139e-02,  5.3637e-01, -9.4398e-01,  4.7519e-01,\n",
       "        -1.1432e-01,  2.1399e-01, -1.4102e-01,  1.4344e-01,  2.4766e-01,\n",
       "         7.8911e-01, -6.4628e-02,  4.0958e-01, -8.1142e-02,  3.6798e-01,\n",
       "        -1.0246e-01,  1.1025e+00,  7.2273e-02, -1.8517e-01, -1.8571e-01,\n",
       "         5.0199e-01, -1.1987e-01,  1.0264e-01,  3.3905e-01, -3.5340e-01,\n",
       "         5.4331e-01, -5.3306e-01,  7.1482e-01, -2.7544e-01,  3.0631e-01,\n",
       "         1.1827e-01, -1.0054e-02,  1.1948e+00,  2.3989e-01,  1.5137e-02,\n",
       "        -2.3533e-01,  5.8331e-01, -4.4614e-01,  6.9828e-02,  2.5727e-01,\n",
       "         1.2196e-01, -2.8813e-01,  1.1927e-01,  2.1705e-02, -3.4713e-01,\n",
       "        -3.0654e-01,  4.6874e-03,  3.9784e-01,  1.7421e-01, -1.6377e-01,\n",
       "        -3.1119e-01, -3.5607e-01,  2.0911e-01,  4.0916e-02,  1.0638e-01,\n",
       "        -1.9917e-01,  3.9811e-01, -4.7166e-01, -8.1484e-01,  2.9909e-01,\n",
       "        -1.4800e-01, -1.3293e-01, -4.3069e-02,  4.6294e-01, -1.8636e-01,\n",
       "         4.9335e-01, -2.0745e-01, -3.3486e-01, -2.5860e-01,  4.3929e-01,\n",
       "         3.0980e-01,  1.5955e-01,  4.5947e-02, -2.0603e-01, -8.7666e-03,\n",
       "         5.2215e-02,  2.7382e-01, -1.5335e-01, -3.5553e-01, -4.2043e-01,\n",
       "        -1.1334e-01, -1.5309e-01,  4.7946e-01, -2.4579e-01, -2.4538e-01,\n",
       "        -1.0197e-02,  1.8421e-01, -6.8009e-04,  2.0684e-02, -5.5230e-02,\n",
       "         5.0582e-01,  5.3211e-01,  1.5725e-01,  6.0574e-02,  9.2153e-02,\n",
       "         8.8122e-02,  2.8314e-01,  7.0268e-01,  4.0574e-01,  3.1250e-01,\n",
       "         3.2603e-01, -1.1768e+00, -2.3811e-02, -1.0360e-01,  7.1549e-02,\n",
       "        -7.7951e-02, -2.5275e-02,  5.8104e-02, -1.0056e+00, -2.0569e-01,\n",
       "        -1.5198e-01, -3.5617e-01, -2.7183e-02, -1.8499e-01, -1.9295e-01,\n",
       "        -9.2337e-01, -3.9401e-01,  1.2258e-01,  4.3216e-01,  5.6573e-01,\n",
       "         5.2585e-02,  7.9351e-02,  3.7099e-02, -3.1976e-01,  1.5335e-01,\n",
       "        -1.4138e-01, -1.9939e-01,  5.0299e-01,  1.6661e-01,  6.1720e-02,\n",
       "         1.9520e-01,  7.9001e-01, -6.2651e-01,  8.7730e-02, -5.6495e-02,\n",
       "         2.3294e-02, -7.8828e-01, -6.1297e-01,  3.2909e-02, -4.0165e-01,\n",
       "        -2.5012e-01, -1.9696e-01,  4.3849e-02,  1.3619e-01, -6.9579e-01,\n",
       "        -3.3597e-02,  6.1513e-02,  3.2764e-01,  7.0836e-02,  5.2525e-01,\n",
       "         1.7136e-01,  2.3154e-01,  7.2538e-02,  2.7403e-01, -6.5781e-02,\n",
       "        -5.7809e-01,  5.4831e-01, -3.0429e-01, -2.2340e-01, -2.3544e-01,\n",
       "        -3.0100e-01,  3.1252e-01,  2.0118e-01, -1.5071e-01,  1.6097e-01,\n",
       "        -1.7932e-01,  2.3010e-01,  5.0228e-01, -3.5673e-01, -8.6041e-02,\n",
       "        -4.3394e-02, -1.3426e-01,  2.1150e-01, -3.1447e-01,  1.0022e-01,\n",
       "        -4.3915e-01, -1.6578e-01,  3.8395e-01, -3.9176e-01, -6.9220e-02,\n",
       "        -3.7654e-01, -4.0216e-01, -8.5690e-02,  7.2750e-01, -3.3792e-01,\n",
       "         3.8918e-01,  6.0003e-01, -7.4008e-02,  2.3888e-01,  1.0067e-02,\n",
       "        -1.5032e-01, -5.8042e-01, -5.3016e-01,  6.6448e-02, -4.1471e-02,\n",
       "        -2.6317e-01, -6.2172e-01, -3.4495e-01,  2.3265e-01, -7.9571e-02,\n",
       "        -7.5969e-02,  3.6022e-01, -2.1569e-01, -4.1720e-01,  1.6833e-01,\n",
       "        -1.6308e-01,  1.0260e-01,  6.9519e-01, -1.3697e-01,  5.2815e-01,\n",
       "         2.7705e-01, -2.4081e-01, -1.1213e-01, -9.1386e-02,  1.9289e-01,\n",
       "        -9.1994e-02,  4.2672e-01, -4.1601e-01,  7.9432e-02,  6.0936e-03,\n",
       "         1.5705e-01,  1.8615e-01, -2.1675e-01,  1.7314e-01,  3.7845e-01,\n",
       "         1.7393e-02, -3.4465e-02, -7.1879e-01, -2.0090e-01,  1.5347e-01,\n",
       "         8.5293e-01, -4.4308e-02, -4.8210e-02, -3.7067e-01,  2.5443e-01,\n",
       "        -2.7288e-01, -2.3671e-01, -8.8140e-02, -1.1183e-01,  9.0636e-02,\n",
       "         2.7588e-01, -1.3678e-01,  5.2899e-02,  4.7217e-01, -1.4338e-01,\n",
       "        -1.7281e-01, -2.7610e-02,  3.4340e-01, -9.8502e-03,  1.3946e-01,\n",
       "         4.5056e-01, -2.8646e-01,  3.4648e-01,  9.7076e-02,  8.7115e-04,\n",
       "         2.4367e-01,  6.4935e-01, -1.8187e-01,  4.4333e-01, -1.5013e-01,\n",
       "        -4.0201e-01, -6.0895e-01,  2.1613e-01, -4.2299e-01, -5.0649e-01,\n",
       "         1.0587e-01,  3.3932e-01, -1.6332e-01, -2.9209e-01,  3.5769e-01,\n",
       "         2.0063e-01,  1.8325e-01, -1.5567e-01,  3.4888e-01,  3.5042e-01,\n",
       "        -4.1962e-01, -1.6107e-01, -7.6194e-01,  2.0464e-01,  3.6038e-01,\n",
       "        -2.5244e-01,  3.9281e-01,  1.3869e-01,  5.6423e-01,  7.6067e-02,\n",
       "         8.9968e-02, -4.9811e-01,  2.4460e-03, -9.2691e-01, -7.3725e-01,\n",
       "         8.1592e-02, -1.0504e-01,  3.0267e-01,  3.9766e-01, -1.6115e-01,\n",
       "         2.9265e-02,  1.5670e-01,  1.4608e-01,  5.8493e-01, -5.5044e-02,\n",
       "        -7.1101e-01, -7.7851e-01,  9.9928e-02, -1.1541e-01,  2.5838e-01,\n",
       "         7.7735e-01,  4.1332e-01,  2.0713e-01, -1.0672e-02,  4.1482e-02,\n",
       "         1.2235e-01, -4.2526e-01,  3.6737e-02, -5.3193e-02, -3.7985e-01,\n",
       "         1.9836e-01,  1.4426e-01,  1.0923e-01, -1.0060e-01,  1.0375e-01,\n",
       "         1.9050e-01,  1.5346e-03, -3.1962e-01,  1.0374e-03, -1.1843e-01,\n",
       "        -1.2016e-02, -1.9410e-01,  6.2292e-02, -7.6368e-01,  1.0209e-01,\n",
       "        -1.6066e-01, -5.0249e-01, -1.0585e-01, -2.7351e-01, -2.1308e-01,\n",
       "         4.4341e-01,  1.4426e-01,  2.0357e-01,  1.2314e-01,  1.0850e-01,\n",
       "         1.6190e-01, -4.4800e-02, -3.5986e-01, -7.3658e-01,  4.8632e-01,\n",
       "         2.6533e-01,  5.4314e-01,  1.5810e-01, -2.9765e-01, -1.0760e-01,\n",
       "         4.8555e-01, -2.9836e-01, -3.8842e-01,  2.3603e-01, -2.3202e-01,\n",
       "        -7.8243e-01,  6.8932e-01, -3.4008e-02,  4.2037e-01,  5.4205e-01,\n",
       "        -3.4006e-01, -1.0588e-01, -8.5521e-02,  6.8164e-01, -5.5592e-01,\n",
       "        -2.2191e-03,  2.5059e-02,  2.0012e-01,  2.4139e-01, -1.8948e-01,\n",
       "         4.0543e-02,  1.4704e-02,  2.7202e-01,  3.7190e-01, -3.9296e-01,\n",
       "         3.8374e-01,  3.3496e-01, -3.4987e-01, -2.4835e-01,  5.6299e-01,\n",
       "         1.2184e-01,  3.5275e-01,  1.4948e-01,  4.7569e-01, -2.7759e-02,\n",
       "         1.2446e-01, -2.9178e-01,  4.3205e-02,  5.6878e-01, -3.5472e-01,\n",
       "         2.0790e-01,  2.8604e-01,  8.5468e-02,  1.4472e-01,  4.4596e-02,\n",
       "         3.1708e-01, -2.8120e-01, -3.2391e-01,  3.0852e-01, -5.9925e-01,\n",
       "        -2.8899e-01, -4.1548e-01, -2.0651e-01, -3.8527e-01,  2.6621e-01,\n",
       "         4.6168e-04,  2.8076e-01, -1.2906e-01, -2.1316e-02,  3.2132e-01,\n",
       "         2.8938e-01, -7.4262e-01,  1.4865e-01,  6.6736e-02, -3.1477e-01,\n",
       "        -4.8628e-01, -1.7180e+00,  6.1380e-02,  9.8117e-02,  1.9838e-01,\n",
       "         1.4626e-01, -2.6119e-01,  6.7206e-02, -7.2593e-02, -2.4075e-01,\n",
       "         3.9433e-02, -9.6558e-01,  2.1609e-01,  6.7765e-02,  5.4470e-01,\n",
       "        -1.6730e-01, -6.6136e-02, -2.2884e-01,  4.3422e-01, -1.1684e-01,\n",
       "        -1.5389e-01,  4.1055e-01, -4.1403e-01, -2.4803e-01, -1.3797e-01,\n",
       "         2.1175e-01,  1.0962e-03,  1.3638e-01,  2.1203e-01,  4.0661e-02,\n",
       "        -1.7170e-01, -4.4395e-01,  5.9372e-01,  3.5123e-01, -2.1288e-01,\n",
       "        -3.5218e-01,  9.6992e-02,  3.3699e-02])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99691a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
